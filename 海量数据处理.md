海量数据处理
---
> #### 常用的数据结构：
> Bloom Filter ：利用位数组表示的一个集合，可以判断该元素是否是属于这个集合，属于一种概率算法，它可以保证数组内的每一项都会被检索，判断元素在集合中的时候会出现一定的错误；若判断该元素不在该集合中，那就一定不在；适用于可以容忍一定错误率的场合；
> 
> Hash : 常用hash算法进行一种压缩映射，压缩作用
> 
> Bit-map (Java 中常用 BitSet)： 用一个 bit位来标记某个元素对应的值；通过 Ox01 << ( i % length ) 去计算元素，
>  
>  堆： 特殊二叉树（堆排序）
>  + 每个节点的值都大于（小于）孩子节点的值；
>  + 树是完全平衡的；
>   
>    trie 树：适用于 前缀和词频统计；
>    
>   外排序：
>   + 首先按内存大小，将外存上含n个记录的文件分成若干长度L的子文件或段。依次读入内存并利用有效的内部排序对他们进行排序，并将排序后得到的有序字文件重新写入外存，通常称这些子文件为归并段；
>   + 对这些归并段进行逐趟归并，使归并段逐渐由小到大，直至得到整个有序文件为之
#### Top k 问题
解法： 分治 + hash / Trie树（字典树） + 小顶堆

> Hash 的特性：可以把无序重复的数据，散列成大致有序的，包含全部重复单元的模块；
> 当出现内存不足，需要分治的时候，hash算法必不可少；

常用方式：
1. 建立 K 节点的最小堆，遍历读取后面的数字，与最小堆的堆顶进行比较；若大于就去顶，插入新数字进行堆调整；若小于就继续往下遍历；最后中序遍历输出堆，空间复杂度是 k 常数级别，时间复杂度 O（nklogk）
2. 优化方法: 当所给数据中有重复数据的时候，采用 hash 表进行一部分的去重，再结合建立最小堆；

##### 使用框架解决问题：
Top k 方法适合 MapReduce 框架 ：
组成 一个 Map 函数，两个 Reduce 函数；
+ Map 通过Hash算法，把重复的数据放入 同一个 Reduce 栈
+ 第一个 Reduce 用HashMap计算栈中每一个数据出现的次数；
+ 第二个 Reduce 利用堆排序 选出全体栈中数据的 Top k 即可； 